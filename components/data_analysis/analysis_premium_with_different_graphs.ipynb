{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This jupyter notebook is for analysing factor premium. Specifically, we design this notebook to plot positive and negative factor premiums, median heatplots etc, for deciding whether to reverse heuristics or not. Please refer to Factor model confluence page to understand better about heuristics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1 import\n",
    "import pandas as pd \n",
    "import glob\n",
    "import os \n",
    "from typing import List\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import Engine\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import datetime\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import openpyxl\n",
    "from openpyxl.styles import Color\n",
    "from openpyxl.formatting.rule import ColorScale, FormatObject, ColorScaleRule\n",
    "from openpyxl.formatting.rule import Rule\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions and data preparation are the two function blocks that are essential for running the rest of other functions. Hence, please run these two function blocks before running other blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sqlalchemy engine\n"
     ]
    }
   ],
   "source": [
    "# Blcok 2 Utility functions\n",
    "url = f\"postgresql://quant_factor:quant_factor@192.168.1.156:5432/quant_dev\"\n",
    "kwargs = dict(pool_size=1, max_overflow=-1, isolation_level=\"AUTOCOMMIT\", pool_pre_ping=True, pool_recycle=600,\n",
    "                      echo=False)\n",
    "\n",
    "def creating_engine():\n",
    "    \"\"\"This function creates a sqlalchemy engine using kwargs and url defined in this cell for query.\n",
    "\n",
    "    Returns:\n",
    "        engine (sqlalchemy.engine.Engine): sqlalchmey engine for connection\n",
    "    \"\"\"\n",
    "    print(f\"Creating sqlalchemy engine\")\n",
    "    engine = create_engine(url,**kwargs)\n",
    "    return engine\n",
    "\n",
    "engine = creating_engine()\n",
    "\n",
    "def read_factor_formula_ratios():\n",
    "    \"\"\"This function reads name, is_active, smb_positive and pillar columns from factor_formula_ratios table.  \n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): Dataframe with 'name' as index\n",
    "    \"\"\"\n",
    "    query = f\"\"\"SELECT name, is_active, smb_positive , pillar FROM factor.factor_formula_ratios;\"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        df = pd.read_sql(query,connection, index_col = ['name'])\n",
    "    return df\n",
    "\n",
    "def read_table_all_from_factor_processed_premium_20_pct(weeks_to_expire:int = 8):\n",
    "    \"\"\"This function all columns from factor_formula_premium_20_pct table. \n",
    "    \n",
    "    Args:\n",
    "        weeks_to_expire (int): Weeks to expire for factor premium, Defaults to 8 \n",
    "        \n",
    "    Returns:\n",
    "        df_20_pct (pd.DataFrame): Dataframe with 'testing_period','field','group','weeks_to_expire','average_days' as index.\n",
    "    \"\"\"\n",
    "    query = 'SELECT * FROM factor.factor_processed_premium_20_pct AND weeks_to_expire = {weeks_to_expire};'\n",
    "    with engine.connect() as connection:\n",
    "        df_20_pct = pd.read_sql(query,connection, index_col = ['testing_period','field','group','weeks_to_expire','average_days'])\n",
    "    return df_20_pct\n",
    "    \n",
    "def read_table_all_from_factor_processed_premium_20_pct_with_specified_market(market: str = 'USD', weeks_to_expire:int = 8):\n",
    "    \"\"\"This function reads all columns from factor_formula_premium_20_pct table with a specific market.\n",
    "\n",
    "   Args:\n",
    "        market (str, optional): Market for factor premiums. Defaults to 'USD'.\n",
    "        weeks_to_expire (int): Weeks to expire for factor premium, Defaults to 8 \n",
    "\n",
    "    Returns:\n",
    "        df_20_pct_specified_mkt (pd.DataFrame): Dataframe with 'testing_period','field','group' as index and 'testing_period','field','group' as columns.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"SELECT * FROM factor.factor_processed_premium_20_pct WHERE \"group\"='{market.upper()}' AND weeks_to_expire = {weeks_to_expire};\"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        print(f\"Reading table all with market = {market.upper()}\")\n",
    "        df_20_pct_specified_mkt = pd.read_sql(query,connection, index_col = ['testing_period','field','group'])\n",
    "        df_20_pct_specified_mkt = df_20_pct_specified_mkt.drop(columns=['weeks_to_expire','average_days', 'updated'])\n",
    "    return df_20_pct_specified_mkt\n",
    "\n",
    "def merge_factor_premium_with_smb_and_retain_active_factor(df_unprocessed: pd.DataFrame = None):\n",
    "    \"\"\"This function merges factor premium with factor formula ratios for smb_positive column and filters out inactive factors. Steps include:\n",
    "\n",
    "        1. [Read] factor formula ratios table for merging\n",
    "        2. [Filter and merge] to remove inactive factors and merge formula ratios table to ratio tables\n",
    "        3. [Multi-indexing] newly formed table with index (testing_periods, field, group and pillar)\n",
    "    Args:\n",
    "        df_unprocessed (pd.DataFrame, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        df_processed (pd.DataFrame): processed dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # 1.[Read]\n",
    "    formula = read_factor_formula_ratios()\n",
    "    formula = formula.reset_index()\n",
    "\n",
    "    # 2. [Filter and merge]\n",
    "    active_factor = formula.loc[(formula['is_active']==True) & ~(formula['smb_positive'].isnull())]['name'].tolist()\n",
    "    formula_merge = formula.loc[(formula['is_active']==True) & ~(formula['smb_positive'].isnull())][['name','smb_positive','pillar']]\n",
    "    df_unprocessed = df_unprocessed.loc[df_unprocessed.index.isin(active_factor,level='field')]\n",
    "    df_unprocessed = df_unprocessed.reset_index()\n",
    "    df_unprocessed = df_unprocessed.merge(formula_merge, how='left', left_on=['field'], right_on=['name'])\n",
    "\n",
    "    # 3. [Multi-indexing]\n",
    "    df_processed = df_unprocessed.set_index(['testing_period','field','group', 'pillar'])\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "def categorizing_larger_between_smaller(df_reversed: pd.DataFrame = None):\n",
    "    \"\"\"This function takes in factor premium dataframe with factor premium reversed according to smb_positive column and categorise the factors into three categories, namely >0.001, <-0.001 and between 0.001 and -0.001.\n",
    "\n",
    "    Args:\n",
    "        df_reversed (pd.DataFrame, optional): Dataframe with factor premium reversed. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        df_categorized: Dataframe with values categorized\n",
    "    \"\"\"\n",
    "    df_reversed = df_reversed.reset_index().set_index(['field','testing_period','group', 'pillar'])\n",
    "    df_categorized = df_reversed.groupby(['testing_period','group', 'field', 'pillar'])['value'].transform(lambda x : '>0.001' if x.sum() >0.001 else '<-0.001' if x.sum() <-0.001 else '[-0.001,0.001]').rename('value')\n",
    "    return df_categorized\n",
    "\n",
    "def reversing_heuristics(df_processed: pd.DataFrame = None):\n",
    "    \"\"\"This function takes in a dataframe with factor ratios and their corresponding smb_positive column and reverse factor ratios' signs according to smb_positive column. Steps include:\n",
    "\n",
    "        1. [Reverse] the sign of factor ratios' values according the smb_positive column. Specifically, if smb_positive is False, we times negative 1 to the existing values. (smb_positive is a bad name. This should be translated into 'Should be small minus big?' If False, then we should flip the sign because our model is currently assuming small minus big method.)\n",
    "\n",
    "    Args:\n",
    "        df_processed (pd.DataFrame, optional): Dataframe with factor ratios and smb_columns. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        df_reversed (pd.DataFrame): Dataframe with the signs reversed according to smb_positive column\n",
    "    \"\"\"\n",
    "    # 1.[Reverse]\n",
    "    df_processed['sign'] = np.where(df_processed['smb_positive']==False,-1, 1)\n",
    "    df_processed['value'] = df_processed['value']*df_processed['sign']\n",
    "    df_reversed = df_processed.drop(columns=['sign','smb_positive'])\n",
    "    return df_reversed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3 Data preparation \n",
    "\n",
    "def data_preparation(weeks_to_expire:int = 8):\n",
    "    \"\"\"Data preparation with specified weeks to expire parameter\n",
    "\n",
    "    Args:\n",
    "        weeks_to_expire (int, optional): Specified weeks to expire. Defaults to 8.\n",
    "\n",
    "    Returns:\n",
    "        df_all (pd.DataFrame): Factor processed premiums for all markets\n",
    "    \"\"\"\n",
    "    df_usd = read_table_all_from_factor_processed_premium_20_pct_with_specified_market('USD', weeks_to_expire)\n",
    "    df_hkd = read_table_all_from_factor_processed_premium_20_pct_with_specified_market('HKD', weeks_to_expire)\n",
    "    df_cny = read_table_all_from_factor_processed_premium_20_pct_with_specified_market('CNY', weeks_to_expire)\n",
    "    df_eur = read_table_all_from_factor_processed_premium_20_pct_with_specified_market('EUR', weeks_to_expire)\n",
    "    df_all = pd.concat([df_usd, df_hkd, df_cny, df_eur])\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below two blocks contain functions blocks that are essential for generating plotly graphs for analysing >0.001, <-0.001 and between for different periods and markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4 Data processing before period and market comparisons\n",
    "def data_processing_for_comparison(df_all: pd.DataFrame = None):\n",
    "    \"\"\"This function takes in factor ratios from USD, HKD, CNY and EUR all concatenated and do the following data processing for market comparison. \n",
    "    1. [Merge and filter] input dataframe with factor formula ratios and filter out inactive factors\n",
    "    2. [Reverse] factor premiums according to smb_positive column\n",
    "    3. [Categorise] factor premiums values into three, namely >0.001, <-0.001 and between 0.001 and -0.001\n",
    "\n",
    "    Args:\n",
    "        df_all (pd.DataFrame, optional): Raw factor formula ratios for all markets. Must contain columns 'testing_period','field','group' and 'values' for subsequent processing. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        df_reversed_and_categorized (pd.DataFrame): Dataframe processed.\n",
    "    \"\"\"\n",
    "    # 1. [Merge and filter]\n",
    "    df_processed = merge_factor_premium_with_smb_and_retain_active_factor(df_all)\n",
    "    # 2. [Reverse]\n",
    "    df_reversed = reversing_heuristics(df_processed=df_processed)\n",
    "    # 3. [Categorise]\n",
    "    df_reversed_and_categorized = categorizing_larger_between_smaller(df_reversed)\n",
    "    return df_reversed_and_categorized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blcok 5 Comparing different periods and different markets for >0.001, <-0.001 and between distribution\n",
    "\n",
    "def for_each_pillar_compare_different_markets(df_grouped: pd.DataFrame = None):\n",
    "    \"\"\"Function for generating plotly graphs that indicate the percentage of factor premiums that are between 0.001 and -0.001, below -0.001 and larger than 0.001 for each pillar different markets and different time periods.Steps include:\n",
    "    \n",
    "    1. [Get] pillars and markets to be loop through\n",
    "    2. [Loop pillar] to get the dataframe that is within that pillar\n",
    "    3. [Loop period] to get the dataframe that is within that period\n",
    "    4  [Calculate percentage] for three categories for each field and pillar using dataframe groupby function\n",
    "    5. [Get fields] that are unique within the dataframe for looping\n",
    "    6. [Loop field] to get the dataframe that is within that field\n",
    "        7. [For each field], plot a subplot for the percentage distribution of three categories for four markets, meaning that there should be four categorry bars for four different markets\n",
    "        8. [Aggregate] the subplots(number of subplots should be equal to number of fields) for each time period\n",
    "        9. [Write] each plot with subplots inside it to html or show it\n",
    "\n",
    "    Args:\n",
    "        df_grouped (pd.DataFrame, optional): Dataframe with all the factor premiums reversed and its values categorized into either >0.001, <-0.001 or between. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. [Get] pillars and markets to be loop through\n",
    "    pillars = sorted(list(df_grouped.index.levels[3]))\n",
    "    markets = sorted(list(df_grouped.index.levels[2]))\n",
    "\n",
    "    # 2. [Loop pillar]\n",
    "    for pillar in pillars:\n",
    "        year_ranges = [datetime.date(2019,1,1),datetime.date(2019,1,1),datetime.datetime.today().date()]\n",
    "\n",
    "        # 3. [Loop period]\n",
    "        for year_range in year_ranges:\n",
    "            df_group = df_grouped.reset_index()\n",
    "            if year_range == datetime.date(2008,1,1):\n",
    "                df_group =  df_group.loc[df_group['testing_period']<year_range].set_index(['field','testing_period', 'group','pillar'])\n",
    "            elif year_range == datetime.date(2019,1,1):\n",
    "                df_group =  df_group.loc[(df_group['testing_period']>datetime.date(2008,1,1))&(df_group['testing_period']<year_range)].set_index(['field','testing_period', 'group','pillar'])\n",
    "            else:\n",
    "                df_group =  df_group.loc[(df_group['testing_period']>datetime.date(2019,1,1))&(df_group['testing_period']<year_range)].set_index(['field','testing_period', 'group','pillar'])\n",
    "\n",
    "            # 4  [Calculate percentage]\n",
    "            df_percent = df_group.groupby(['field','group', 'pillar']).value_counts(normalize=True).rename('value') # normalize to give percentage\n",
    "            \n",
    "            # 5. [Get fields]\n",
    "            fields_in_pillar = sorted(list(df_percent.loc[df_percent.index.isin([pillar],level='pillar')].index.get_level_values('field').drop_duplicates()))\n",
    "    \n",
    "            i=1\n",
    "            j=1\n",
    "            fig = make_subplots(rows=3, cols=10, subplot_titles=fields_in_pillar)\n",
    "            show_legend=True\n",
    "            df = df_percent\n",
    "\n",
    "            # 6. [Loop field]\n",
    "            for field in fields_in_pillar:\n",
    "                if i==1 and j==1:\n",
    "                    show_legend=True\n",
    "                else:\n",
    "                    show_legend=False\n",
    "\n",
    "                \n",
    "                \n",
    "                bigger_than_pos_001 = df.loc[df.index.isin(['>0.001'],level='value')&(df.index.isin([pillar],level='pillar'))&(df.index.isin([field],level='field'))]*100\n",
    "\n",
    "\n",
    "                between = df.loc[df.index.isin(['[-0.001,0.001]'],level='value')&(df.index.isin([pillar],level='pillar'))&(df.index.isin([field],level='field'))]*100\n",
    "\n",
    "                between_hkd = between.loc[between.index.isin(['HKD'],level='group')]\n",
    "                between_cny = between.loc[between.index.isin(['CNY'],level='group')]\n",
    "                between_usd = between.loc[between.index.isin(['USD'],level='group')]\n",
    "                between_eur = between.loc[between.index.isin(['EUR'],level='group')]\n",
    "\n",
    "                between.loc[(field, 'HKD', pillar, '[-0.001,0.001]')] = 0 if len(between_hkd)==0 else between_hkd[0]\n",
    "                between.loc[(field, 'CNY', pillar, '[-0.001,0.001]')]  = 0 if len(between_cny)==0 else between_cny[0]\n",
    "                between.loc[(field, 'USD', pillar, '[-0.001,0.001]')]  = 0 if len(between_usd)==0 else between_usd[0]\n",
    "                between.loc[(field, 'EUR', pillar, '[-0.001,0.001]')]  = 0 if len(between_eur)==0 else between_eur[0]\n",
    "\n",
    "                between = between.sort_index(level='group')\n",
    "\n",
    "                less_than_neg_001 = df.loc[df.index.isin(['<-0.001'],level='value')&(df.index.isin([pillar],level='pillar'))&(df.index.isin([field],level='field'))]*100\n",
    "\n",
    "                # 7. [For each field]\n",
    "                fig.add_trace(go.Bar(x=['CNY', 'EUR', 'HKD', 'USD'], y=less_than_neg_001.tolist(), name='<-0.001', showlegend=show_legend, legendgroup='<-0.001',marker_color='red'), row=i, col =j)\n",
    "                fig.add_trace(go.Bar(x=['CNY', 'EUR', 'HKD', 'USD'], y=between.tolist(), name='[-0.001,0.001]', showlegend=show_legend, legendgroup='[-0.001,0.001]',marker_color='blue'), row=i, col =j)\n",
    "                fig.add_trace(go.Bar(x=['CNY', 'EUR', 'HKD', 'USD'], y=bigger_than_pos_001.tolist(), name='>0.001', showlegend=show_legend, legendgroup='>0.001',marker_color='yellow'), row=i, col =j)\n",
    "                fig.update_layout(barmode='stack')\n",
    "\n",
    "                if j>=9:\n",
    "                    j=1\n",
    "                    i = i+1\n",
    "                else:    \n",
    "                    j = j+1\n",
    "\n",
    "            # 8. [Aggregate]\n",
    "            if year_range == datetime.date(2019,1,1):\n",
    "                text = 'before 2008'\n",
    "            elif year_range ==datetime.date(2019,1,1):\n",
    "                text = 'between 2008 and 2019'\n",
    "            else:\n",
    "                text = 'after 2019'\n",
    "\n",
    "            # 9. [Write]\n",
    "            fig.update_layout(height=2000, width=2100, title_text=f\"Factor premium for different markets for pillar {pillar.upper()} \"+text)\n",
    "            # fig.show()\n",
    "            fig.write_html(f\"./market_comparison/market_comparison_{pillar}_{year_range}.html\")\n",
    "\n",
    "\n",
    "def for_each_pillar_compare_different_periods(df_grouped: pd.DataFrame = None):\n",
    "    \"\"\"Function for generating plotly graphs that indicate the percentage of factor premiums that are between 0.001 and -0.001, below -0.001 and larger than 0.001 for each pillar, each market and different time periods. Steps include:\n",
    "    \n",
    "    1. [Get] pillars and markets to be loop through\n",
    "    2. [Loop pillar] to get the dataframe that is within that pillar\n",
    "    3. [Loop market] to get the dataframe that is within that period \\n\n",
    "    For each market we generate one plot with subplots in it by steps below\n",
    "        4. [Get fields] that are unique within the dataframe for looping\n",
    "        5. [Loop field] to get the dataframe that is within that field\n",
    "        6. [For each field]\n",
    "            a. [Get different periods] for dataframe\n",
    "            b. [Calculate percentage] for three categories for each period\n",
    "            c. [Subplot] with three category bars (each bar for each period)\n",
    "        7. [Aggregate] the subplots(number of subplots should be equal to number of fields) for each time period\n",
    "        8. [Write] each plot with subplots inside it to html or show it\n",
    "\n",
    "    Args:\n",
    "        df_grouped (pd.DataFrame, optional): Dataframe with all the factor premiums reversed and its values categorized into either >0.001, <-0.001 or between. Defaults to None.\n",
    "    \"\"\"\n",
    "    # 1. [Get]\n",
    "    pillars = sorted(list(df_grouped.index.levels[3]))\n",
    "    markets = sorted(list(df_grouped.index.levels[2]))\n",
    "\n",
    "    # 2. [Loop pillar]\n",
    "    for pillar in pillars:\n",
    "        df_group = df_grouped.loc[df_grouped.index.isin([pillar],level='pillar')]\n",
    "        # 3. [Loop market]\n",
    "        for market in markets:\n",
    "            df_group_market = df_group.loc[df_group.index.isin([market],level='group')]\n",
    "\n",
    "            # 4. [Get fields]\n",
    "            fields_in_pillar = sorted(df_group_market.reset_index()['field'].drop_duplicates().tolist())\n",
    "\n",
    "            i=1\n",
    "            j=1\n",
    "\n",
    "            fig = make_subplots(rows=3, cols=10, subplot_titles=fields_in_pillar)\n",
    "            show_legend=True\n",
    "\n",
    "            # 5. [Loop field]\n",
    "            for field in fields_in_pillar:\n",
    "                if i==1 and j==1:\n",
    "                    show_legend=True\n",
    "                else:\n",
    "                    show_legend=False\n",
    "        \n",
    "                df_group_field = df_group_market.loc[df_group_market.index.isin([field],level='field')]\n",
    "                df_group_year = df_group_field.reset_index()\n",
    "\n",
    "                # 6.a [Get different periods]\n",
    "                df_group_before_2008_for_each_field =  df_group_year.loc[df_group_year['testing_period']<datetime.date(2008,1,1)].set_index(['field','testing_period', 'group','pillar']) \n",
    "                df_group_between_for_each_field =  df_group_year.loc[(df_group_year['testing_period']>datetime.date(2008,1,1))&(df_group_year['testing_period']<datetime.date(2019,1,1))].set_index(['field','testing_period', 'group','pillar'])\n",
    "                df_group_after_for_each_field =  df_group_year.loc[(df_group_year['testing_period']>datetime.date(2019,1,1))&(df_group_year['testing_period']<datetime.datetime.today().date())].set_index(['field','testing_period', 'group','pillar'])\n",
    "\n",
    "                # 6.b [Calculate percentage] \n",
    "                df_group_before_2008_total_percentage = df_group_before_2008_for_each_field.groupby(['field']).value_counts(normalize=True).rename('value')   # normalize to give percentage\n",
    "                df_group_between_total_percentage = df_group_between_for_each_field.groupby(['field']).value_counts(normalize=True).rename('value')  \n",
    "                df_group_after_total_percentage = df_group_after_for_each_field.groupby(['field']).value_counts(normalize=True).rename('value')  \n",
    "\n",
    "                # 6.c [Subplot]\n",
    "                val_bigger_than_pos_001_before_2008 = df_group_before_2008_total_percentage.loc[df_group_before_2008_total_percentage.index.isin(['>0.001'],level='value')]\n",
    "                val_bigger_than_pos_001_between_2008_2019 = df_group_between_total_percentage.loc[df_group_between_total_percentage.index.isin(['>0.001'],level='value')]\n",
    "                val_bigger_than_pos_001_after = df_group_after_total_percentage.loc[df_group_after_total_percentage.index.isin(['>0.001'],level='value')]\n",
    "\n",
    "                val_less_than_neg_001_before_2008 = df_group_before_2008_total_percentage.loc[df_group_before_2008_total_percentage.index.isin(['<-0.001'],level='value')]\n",
    "                val_less_than_neg_001_between_2008_2019 = df_group_between_total_percentage.loc[df_group_between_total_percentage.index.isin(['<-0.001'],level='value')]\n",
    "                val_less_than_neg_001_after = df_group_after_total_percentage.loc[df_group_after_total_percentage.index.isin(['<-0.001'],level='value')]\n",
    "\n",
    "                val_between_before_2008 = df_group_before_2008_total_percentage.loc[df_group_before_2008_total_percentage.index.isin(['[-0.001,0.001]'],level='value')]\n",
    "                val_between_between_2008_2019 = df_group_between_total_percentage.loc[df_group_between_total_percentage.index.isin(['[-0.001,0.001]'],level='value')]\n",
    "                val_between_after = df_group_after_total_percentage.loc[df_group_after_total_percentage.index.isin(['[-0.001,0.001]'],level='value')]\n",
    "\n",
    "                val_between_before_2008_len = len(val_between_before_2008)\n",
    "                val_between_between_2008_2019_len = len(val_between_between_2008_2019)\n",
    "                val_between_after_len = len(val_between_after)\n",
    "\n",
    "\n",
    "                bigger_than_pos_001 = [val_bigger_than_pos_001_before_2008[0],val_bigger_than_pos_001_between_2008_2019[0],val_bigger_than_pos_001_after[0]]\n",
    "                less_than_neg_001 = [val_less_than_neg_001_before_2008[0],val_less_than_neg_001_between_2008_2019[0],val_less_than_neg_001_after[0]]\n",
    "                \n",
    "\n",
    "                val_between_before_2008 = 0 if val_between_before_2008_len == 0 else val_between_before_2008[0]\n",
    "                val_between_between_2008_2019 = 0 if val_between_between_2008_2019_len == 0 else val_between_between_2008_2019[0]\n",
    "                val_between_after = 0 if val_between_after_len == 0 else val_between_after[0]\n",
    "\n",
    "                between = [val_between_before_2008, val_between_between_2008_2019, val_between_after]\n",
    "\n",
    "                fig.add_trace(go.Bar(x=['Before 2008', 'Between 2008 and 2019', 'After 2019'], y=less_than_neg_001, name='<-0.001', showlegend=show_legend, legendgroup='<-0.001',marker_color='red'), row=i, col =j)\n",
    "                fig.add_trace(go.Bar(x=['Before 2008', 'Between 2008 and 2019', 'After 2019'], y=between, name='[-0.001,0.001]', showlegend=show_legend, legendgroup='[-0.001,0.001]',marker_color='blue'), row=i, col =j)\n",
    "                fig.add_trace(go.Bar(x=['Before 2008', 'Between 2008 and 2019', 'After 2019'], y=bigger_than_pos_001, name='>0.001', showlegend=show_legend, legendgroup='>0.001',marker_color='yellow'), row=i, col =j)\n",
    "                fig.update_layout(barmode='stack')\n",
    "\n",
    "                if j>=10:\n",
    "                    j=1\n",
    "                    i = i+1\n",
    "                else:    \n",
    "                    j = j+1\n",
    "\n",
    "            # 7. [Aggregate]\n",
    "            fig.update_layout(height=2000, width=2100, title_text=f\"Factor premium for different year periods for pillar {pillar.upper()} for market {market.upper()}\")\n",
    "\n",
    "            # 8. [Write]\n",
    "            fig.write_html(f\"./period_comparison/periods_comparison_{pillar}_{market}.html\")\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below two blocks contain functions for plotting histogram for probability vs value for different markets. Blcok 7 is the main function that plots the histogram with speicified start_date and end_date, field and title. Hence to run block 7 we need to run block 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6 Utility function for pylotting histogram \n",
    "def plot_histogram(data: pd.DataFrame = None, bin: int = 20, title: str = None, x_title: str = None, mean: pd.DataFrame = None, median: pd.DataFrame =None):\n",
    "    \"\"\"This function plots histogram of values and their probability for four different markets with lines indicating the mean and median position.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame, optional): Dataframe with values. Defaults to None.\n",
    "        bin (int, optional): Bin for value axis (x-axis). Defaults to 20.\n",
    "        title (str, optional): Title for the histogram. Defaults to None.\n",
    "        x_title (str, optional): Name of dataframe column whose values are to be plot. Meaning that it is the x-axis value. Defaults to None.\n",
    "        mean (pd.DataFrame, optional): Mean for each market. Defaults to None.\n",
    "        median (pd.DataFrame, optional): Median for each market. Defaults to None.\n",
    "    \"\"\"\n",
    "    fig = px.histogram(data, nbins=bin, title=title, x=x_title, histnorm='probability', color='group')\n",
    "    mean_hkd = mean.loc[mean.index.isin(['HKD'],level='group')].values[0][0]\n",
    "    mean_cny = mean.loc[mean.index.isin(['CNY'],level='group')].values[0][0]\n",
    "    mean_eur = mean.loc[mean.index.isin(['EUR'],level='group')].values[0][0]\n",
    "    mean_usd = mean.loc[mean.index.isin(['USD'],level='group')].values[0][0]\n",
    "    \n",
    "    median_hkd = median.loc[median.index.isin(['HKD'],level='group')].values[0][0]\n",
    "    median_cny = median.loc[median.index.isin(['CNY'],level='group')].values[0][0]\n",
    "    median_eur = median.loc[median.index.isin(['EUR'],level='group')].values[0][0]\n",
    "    median_usd = median.loc[median.index.isin(['USD'],level='group')].values[0][0]\n",
    "    \n",
    "    dmax = data['value'].max()\n",
    "    dmin = data['value'].min()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=[mean_hkd,mean_hkd], \n",
    "                         y=[dmin,dmax], \n",
    "                         mode='lines', \n",
    "                         line=dict(color='green', width=1, dash='dash'),\n",
    "                         name=f'HKD_mean_{mean_hkd}'))\n",
    "    fig.add_trace(go.Scatter(x=[mean_usd,mean_usd], \n",
    "                         y=[dmin,dmax], \n",
    "                         mode='lines', \n",
    "                         line=dict(color='green', width=1, dash='dash'),\n",
    "                         name=f'USD_mean_{mean_usd}'))\n",
    "    fig.add_trace(go.Scatter(x=[mean_cny,mean_cny], \n",
    "                         y=[dmin,dmax], \n",
    "                         mode='lines', \n",
    "                         line=dict(color='green', width=1, dash='dash'),\n",
    "                         name=f'CNY_mean_{mean_cny}'))\n",
    "    fig.add_trace(go.Scatter(x=[mean_eur,mean_eur], \n",
    "                         y=[dmin,dmax], \n",
    "                         mode='lines', \n",
    "                         line=dict(color='green', width=1, dash='dash'),\n",
    "                         name=f'EUR_mean_{mean_eur}'))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=[median_hkd,median_hkd], \n",
    "                         y=[dmin,dmax], \n",
    "                         mode='lines', \n",
    "                         line=dict(color='red', width=1, dash='dash'),\n",
    "                         name=f'HKD_median_{median_hkd}'))\n",
    "    fig.add_trace(go.Scatter(x=[median_usd,median_usd], \n",
    "                         y=[dmin,dmax], \n",
    "                         mode='lines', \n",
    "                         line=dict(color='red', width=1, dash='dash'),\n",
    "                         name=f'USD_median_{median_usd}'))\n",
    "    fig.add_trace(go.Scatter(x=[median_cny,median_cny], \n",
    "                         y=[dmin,dmax], \n",
    "                         mode='lines', \n",
    "                         line=dict(color='red', width=1, dash='dash'),\n",
    "                         name=f'CNY_median_{median_cny}'))\n",
    "    fig.add_trace(go.Scatter(x=[median_eur,median_eur], \n",
    "                         y=[dmin,dmax], \n",
    "                         mode='lines', \n",
    "                         line=dict(color='red', width=1, dash='dash'),\n",
    "                         name=f'EUR_median_{median_eur}'))\n",
    "\n",
    "    fig.write_html(f'./normal_distribution/{title}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7 Plotting histogram distribution for probability vs value for different markets\n",
    "def plot_histogram_for_different_markets_between(df_reversed: pd.DataFrame= None, field:str = 'ebitda_ev', start_date: datetime.date = datetime.date(2000,1,1), end_date: datetime.date = datetime.date(2023,1,1)):\n",
    "    \"\"\"This function plots histogram for a particular field within a particular time period for USD, CNY, HKD and EUR markets using Plotly. The plot includes a histogram with y axis equals to the probability and x axis the corresponding value. The graph also contains lines indicating median and mean position. \n",
    "\n",
    "    Args:\n",
    "        df_reversed (pd.DataFrame, optional): Reversed Dataframe\n",
    "        field (str, optional): Factor field to be plotted. Defaults to 'ebitda_ev'.\n",
    "        start_date (datetime.date, optional): Starting date of the factor premiums. Defaults to datetime.date(2000,1,1).\n",
    "        end_date (datetime.date, optional): Ending date of the factor premiums.. Defaults to datetime.date(2023,1,1).\n",
    "    \"\"\"\n",
    "    df = df_reversed.reset_index()\n",
    "    df = df.loc[(df['field']==field)&(df['testing_period']>start_date)&(df['testing_period']<end_date)][['value','group']]\n",
    "\n",
    "\n",
    "    \n",
    "    mean = df.set_index('group').groupby('group').mean()\n",
    "    median = df.set_index('group').groupby('group').median()\n",
    "\n",
    "    mean['value'] = mean['value'].apply(lambda x: np.format_float_positional(x,3,False,False,'k'))\n",
    "    median['value'] = median['value'].apply(lambda x: np.format_float_positional(x,3,False,False,'k'))\n",
    "\n",
    "    plot_histogram(df, title=f'Histogram_for_{field}_between_{start_date}_and_{end_date}', bin=400, x_title='value',mean=mean, median=median)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below blcok 8 contains functions to generate median heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8 median heatmap production function\n",
    "\n",
    "def excel_writer(file_name:str = None):\n",
    "    \"Return workbook object with defined filename\"\n",
    "    wb = openpyxl.load_workbook(file_name)\n",
    "    return wb\n",
    "\n",
    "def excel_conditional_formatting(data, ws, num_fields):\n",
    "    \"\"\"Function for excel conditional formatting. Steps include:\n",
    "\n",
    "        1. [Rewrite] market names\n",
    "        2. [Cell position] range for conditional formatting to be applied. num_fields is for calculating the position of the final row in the xlsx file.\n",
    "        3. [Maximum finding] for each market in the data dataframe for setting the color scale of conditional formatting.\n",
    "        4. [Apply] conditional formatting for each currency with different start and final cell positions. Specifically, we apply maximum and -maximum and the max and min for conditional formatting, with 0 as the mid value.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Dataframe for finding the maximum value for each market\n",
    "        ws (worksheet object): openyxl worksheet object\n",
    "        num_fields (int): Number of fields for calculating the position of the final row\n",
    "    \"\"\"\n",
    "    # 1. [Rewrite]\n",
    "    ws['B2']='HKD'\n",
    "    ws['E2']='CNY'\n",
    "    ws['H2']='USD'\n",
    "    ws['K2']='EUR'\n",
    "\n",
    "    for col in ['A','B','C','D','E','F','G','H','I','J','K','L','M']:\n",
    "        ws.column_dimensions[col].width = 5\n",
    "\n",
    "    # 2. [Cell position]\n",
    "    currencies = [('HKD',f\"B5:D{5+num_fields-1}\"),('CNY',f\"E5:G{5+num_fields-1}\"),('USD',f\"H5:J{5+num_fields-1}\"),('EUR',f\"K5:M{5+num_fields-1}\")]\n",
    "\n",
    "    # 3. [Maximum finding]\n",
    "    max_of_all = data.T.groupby('group').max().max(axis=1).rename(index={'aHKD': 'HKD','bCNY': 'CNY','cUSD': 'USD','dEUR': 'EUR'})\n",
    "\n",
    "    # 4. [Apply]\n",
    "    for currency in currencies:\n",
    "\n",
    "        maximum = max_of_all[currency[0]] \n",
    "        \n",
    "        rule = ColorScaleRule(start_type='num', start_value=maximum, start_color='FF0000',\n",
    "                                mid_type='num', mid_value=0, mid_color='FFFFFF',                     end_type='num', end_value=-maximum, end_color='228B22')\n",
    "\n",
    "        ws.conditional_formatting.add(currency[1], rule)\n",
    "\n",
    "def change_name(df):\n",
    "    \"Renaming column names for desired positioning\"\n",
    "    df['group'] = df['group'].replace(['HKD','CNY','USD','EUR'],['aHKD','bCNY','cUSD','dEUR'])\n",
    "    return df\n",
    "\n",
    "def heatmap_median_for_different_markets(file_name:str = None, df_reversed: pd.DataFrame = None, pillar:str = None, sheetname:str = None):\n",
    "    \"\"\"Plotting heuristic research heatmap for specified pillar. Steps include:\n",
    "\n",
    "    1. [Partition] dataframe into three time periods\n",
    "    For each time period\n",
    "    2. [Calculate median] and round up numbers to three significant figures\n",
    "    3. [Write] dataframe into xlsx file\n",
    "    4. [Conditional formatting] for each market\n",
    "    \n",
    "    Args:\n",
    "        file_name (str, optional): File path to write the xlsx file. Defaults to None.\n",
    "        df_reversed (pd.DataFrame, optional): Dataframe with factor premium value reversed. Defaults to None.\n",
    "        pillar (str, optional): Specified pillar. Defaults to None.\n",
    "        sheetname (str, optional): The sheetname to write the dataframe into\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df_reversed.reset_index()\n",
    "    df = change_name(df)\n",
    "    df = df.loc[df['pillar']==pillar]\n",
    "    num_fields = len(df['field'].drop_duplicates())\n",
    "    \n",
    "    # 1. [Partition] dataframe into three time periods\n",
    "    year_ranges = [(datetime.date(1998,1,1),datetime.date(2008,1,1),'between_1998_2008'), (datetime.date(2008,1,1), datetime.date(2019,1,1), 'between_2008_2019'), (datetime.date(2019,1,1), datetime.date(2023,1,1), 'between_2019_2022')]\n",
    "\n",
    "    dataframe_with_different_year_ranges = [(df.loc[(df['testing_period']>year_range[0])&(df['testing_period']<year_range[1])][['value','group','field']],year_range[2]) for year_range in year_ranges]\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    # 2. [Calculate median] and round up numbers to three significant figures\n",
    "    for tup in dataframe_with_different_year_ranges:\n",
    "        df = tup[0]\n",
    "        period = tup[1]\n",
    "        df = df.set_index('group','field').groupby(['group','field'])['value'].median().to_frame()\n",
    "        df['period'] = period\n",
    "        df['value'] = df['value'].apply(lambda x: np.format_float_positional(x,3,False,False,'k'))\n",
    "        df_list.append(df)\n",
    "\n",
    "    data = pd.concat(df_list).reset_index().pivot(index=['field'],columns=['group','period'],values=['value'])\n",
    "    data = data.T.reset_index().set_index(['level_0','group','period']).sort_index().T\n",
    "    data = data.astype(float)\n",
    "\n",
    "    # 3. [Write] dataframe into xlsx file\n",
    "    with pd.ExcelWriter(file_name, mode='a',if_sheet_exists='replace', engine='openpyxl') as writer:\n",
    "        data.to_excel(writer, sheet_name=sheetname)\n",
    "\n",
    "    workbook = excel_writer(file_name)\n",
    "    ws=workbook[sheetname]\n",
    "\n",
    "    # 4. [Conditional formatting]\n",
    "    excel_conditional_formatting(data, ws, num_fields)\n",
    "    workbook.save(file_name)\n",
    "    workbook.close()\n",
    "\n",
    "def median_heat_map_for_all_pillars_and_all_weeks():\n",
    "    \"\"\"This function generates median heat map for all pillars and all weeks. Steps include:\n",
    "        1. [Create] an empty xlsx file called 'median_heatmap.xlsx' in directory median_color_map if not exist for writing the data\n",
    "        2. [Preparing iteration list] for all pillars for different weeks\n",
    "        3. [Loop each pillar and weeks_to_expire combination]\n",
    "        4. [Data preparation] Specifically, generate reversed-heuristic dataframe for specified weeks_to_expire.\n",
    "        5. [Generate heatmap] and write into separate sheet\n",
    "    \"\"\"\n",
    "    file_to_write = f'./median_color_map/median_heatmap.xlsx'\n",
    "\n",
    "    # 1. [Create]\n",
    "    if os.path.isfile(file_to_write):\n",
    "        pass\n",
    "    else:\n",
    "        wb = openpyxl.Workbook()\n",
    "        wb.save(file_to_write)\n",
    "\n",
    "\n",
    "    # 2. [Preparing iteration list]\n",
    "    iteration_list = [[2,4,8,26,52],['quality','value','momentum']]\n",
    "\n",
    "    # 3. [Loop each pillar and weeks_to_expire combination]\n",
    "    for element in itertools.product(*iteration_list):\n",
    "        pillar = element[1]\n",
    "        weeks_to_expire = element[0]\n",
    "\n",
    "        # 4. [Data preparation]\n",
    "        df_all = data_preparation(element[0])\n",
    "        df_processed = merge_factor_premium_with_smb_and_retain_active_factor(df_all)\n",
    "        df_reversed = reversing_heuristics(df_processed=df_processed)\n",
    "\n",
    "        # 5. [Generate heatmap]\n",
    "        heatmap_median_for_different_markets(file_to_write, df_reversed, pillar, f\"{pillar}_{weeks_to_expire}_weeks\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Demonstration and example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Period* comparison for >0.001, <-0.001 and between\n",
    "We first prepare the needed df_reversed for specified weeks_to_expire. Then, we get the reversed and categorized(i.e. >0.001, <-0.001 and between) factor premium and execute plot generating function in **block 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Period comparison\n",
    "df_all = data_preparation(8)\n",
    "df_processed = merge_factor_premium_with_smb_and_retain_active_factor(df_all)\n",
    "df_reversed = reversing_heuristics(df_processed=df_processed)\n",
    "df_reversed_and_categorized = data_processing_for_comparison(df_all)\n",
    "for_each_pillar_compare_different_periods(df_reversed_and_categorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Market* comparison for >0.001, <-0.001 and between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market comparison\n",
    "for_each_pillar_compare_different_markets(df_reversed_and_categorized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram for specified field and start_date/end_date for different markets. \n",
    "We first prepare the needed df_reversed for specified weeks_to_expire and run plot histogram functions in block 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histogram for different markets for different fields\n",
    "df_all = data_preparation(8)\n",
    "df_processed = merge_factor_premium_with_smb_and_retain_active_factor(df_all)\n",
    "df_reversed = reversing_heuristics(df_processed=df_processed)\n",
    "\n",
    "plot_histogram_for_different_markets_between(df_reversed,'market_cap_usd', start_date=datetime.date(2019,1,1), end_date=datetime.date(2023,1,1))\n",
    "\n",
    "plot_histogram_for_different_markets_between(df_reversed,'stock_return_r12_7', start_date=datetime.date(2019,1,1), end_date=datetime.date(2023,1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To generate median heatmaps for all pillars and all weeks, we run function in block 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing heatmap for all pillars and all weeks\n",
    "median_heat_map_for_all_pillars_and_all_weeks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('factor-clustering')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64bb343170ff9069bb97eb08fbc2a6e6a651d7677440b573165e544639dd1d87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
